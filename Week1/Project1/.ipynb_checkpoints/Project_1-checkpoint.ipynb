{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walter Thornton working with Dwayne Kennemore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dwayne. Solved our issue. Completely.\n",
    "    Things to do:\n",
    "        1.6 Alternate metric code\n",
    "        1.7 Answer question\n",
    "        \n",
    "        Delete this cell. Send me a copy.\n",
    "        \n",
    "        IMPORTANT Certain cells cannot be run over and over without clearing some previous output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A: Homework 1\n",
    "\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine\n",
    "\n",
    "---\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "**WARNING**: There is web page scraping in this homework. It takes about 40 minutes. **Do not wait till the last minute** to do this homework.\n",
    "\n",
    "- To submit your assignment follow the instructions given in canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. There is an important CAVEAT to this. DO NOT run the web-page fetching cells again. (We have provided hints like `# DO NOT RERUN THIS CELL WHEN SUBMITTING` on some of the cells where we provide the code). Instead load your data structures from the JSON files we will ask you to save below. Otherwise you will be waiting for a long time. (Another reason to not wait until the last moment to submit.)\n",
    "\n",
    "- Do not include your name in the notebook.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Rihanna or Mariah?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Billboard Magazine puts out a top 100 list of \"singles\" every week. Information from this list, as well as that from music sales, radio, and other sources is used to determine a top-100 \"singles\" of the year list. A **single** is typically one song, but sometimes can be two songs which are on one \"single\" record.\n",
    "\n",
    "In this homework you will: \n",
    "\n",
    "1. Scrape Wikipedia to obtain infprmation about the best singers and groups from each year (distinguishing between the two groups) as determined by the Billboard top 100 charts. You will have to clean this data. Along the way you will learn how to save data in json files to avoid repeated scraping. \n",
    "2. Scrape Wikipedia to obtain information on these singers. You will have to scrape the web pages, this time using a cache to guard against network timeouts (or your laptop going to sleep). You will again clean the data, and save it to a json file.\n",
    "3. Use pandas to represent these two datasets and merge them.\n",
    "4. Use the individual and merged datasets to visualize the performance of the artists and their songs. We have kept the amount of analysis limited here for reasons of time; but you might enjoy exploring music genres and other aspects of the music business you can find on these wikipedia pages at your own leisure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have worked through Lab0 and Lab 1, and Lecture 2.  Lab 2 will help as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, first we import the necessary libraries.  In particular, we use [Seaborn](http://stanford.edu/~mwaskom/software/seaborn/) to give us a nicer default color palette, with our plots being of large (`poster`) size and with a white-grid background. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as pl\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Scraping Wikipedia for Billboard Top 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question you will scrape Wikipedia for the Billboard's top 100 singles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Wikipedia for Billboard singles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using  [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/), and suggest that you use Python's built in `requests` library to fetch the web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Parsing the Billboard Wikipedia page for 1970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the web page at http://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_1970 using a HTTP GET request. From this web page we'll extract the top 100 singles and their rankings. Create a list of dictionaries, 100 of them to be precise, with entries like \n",
    "\n",
    "`{'url': '/wiki/Sugarloaf_(band)', 'ranking': 30, 'band_singer': 'Sugarloaf', 'title': 'Green-Eyed Lady'}`. \n",
    "\n",
    "If you look at that web page, you'll see a link for every song, from which you can get the `url` of the singer or band. We will use these links later to scrape information about the singer or band. From the listing we can also get the band or singer name `band_singer`, and `title` of the song.\n",
    "\n",
    "*HINT: look for a table with class `wikitable`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\home\\appdata\\local\\programs\\python\\python36-32\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mdo_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1317\u001b[0m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[1;32m-> 1318\u001b[1;33m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0m\u001b[0;32m   1319\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# timeout error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\home\\appdata\\local\\programs\\python\\python36-32\\lib\\http\\client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1238\u001b[0m         \u001b[1;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\home\\appdata\\local\\programs\\python\\python36-32\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1284\u001b[0m             \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'body'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1285\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\home\\appdata\\local\\programs\\python\\python36-32\\lib\\http\\client.py\u001b[0m in \u001b[0;36mendheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1233\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1234\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\home\\appdata\\local\\programs\\python\\python36-32\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1025\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\home\\appdata\\local\\programs\\python\\python36-32\\lib\\http\\client.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    963\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 964\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    965\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\home\\appdata\\local\\programs\\python\\python36-32\\lib\\http\\client.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1392\u001b[1;33m             \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\home\\appdata\\local\\programs\\python\\python36-32\\lib\\http\\client.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    935\u001b[0m         self.sock = self._create_connection(\n\u001b[1;32m--> 936\u001b[1;33m             (self.host,self.port), self.timeout, self.source_address)\n\u001b[0m\u001b[0;32m    937\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetsockopt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIPPROTO_TCP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTCP_NODELAY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\home\\appdata\\local\\programs\\python\\python36-32\\lib\\socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    723\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 724\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    725\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\home\\appdata\\local\\programs\\python\\python36-32\\lib\\socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    712\u001b[0m                 \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 713\u001b[1;33m             \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    714\u001b[0m             \u001b[1;31m# Break explicitly a reference cycle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3c003bf7d7f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#retrieves our data from wikipedia\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_1970'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m    \u001b[0mwiki_chart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwiki_chart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lxml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'table'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'wikitable sortable'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\home\\appdata\\local\\programs\\python\\python36-32\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\home\\appdata\\local\\programs\\python\\python36-32\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[1;31m# post-process response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\home\\appdata\\local\\programs\\python\\python36-32\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    542\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[1;32m--> 544\u001b[1;33m                                   '_open', req)\n\u001b[0m\u001b[0;32m    545\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\home\\appdata\\local\\programs\\python\\python36-32\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    502\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    505\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\home\\appdata\\local\\programs\\python\\python36-32\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1359\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[1;32m-> 1361\u001b[1;33m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[0m\u001b[0;32m   1362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m         \u001b[0mhttps_request\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\home\\appdata\\local\\programs\\python\\python36-32\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mdo_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1318\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0;32m   1319\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# timeout error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1320\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1321\u001b[0m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mURLError\u001b[0m: <urlopen error [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond>"
     ]
    }
   ],
   "source": [
    "#retrieves our data from wikipedia\n",
    "with urllib.request.urlopen('https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_1970') as response:\n",
    "   wiki_chart = response.read()\n",
    "soup = BeautifulSoup(wiki_chart, \"lxml\")\n",
    "tables = soup.find('table', attrs={'class':'wikitable sortable'})\n",
    "\n",
    "#scrapes the page for relevant information\n",
    "song_list = []\n",
    "tr_list = tables.find_all('tr')\n",
    "for tr in tr_list:\n",
    "    td_list = tr.find_all('td')\n",
    "    if td_list == [] :\n",
    "        td_list = []\n",
    "    else : \n",
    "        ranking = td_list[0].get_text()\n",
    "        title = td_list[1].get_text()\n",
    "        band_singer = td_list[2].get_text()\n",
    "        soup_of_link = BeautifulSoup(str(td_list), \"lxml\")\n",
    "        url = td_list[2].a[\"href\"]\n",
    "        #creates a dictionary entry for the scraped data\n",
    "        dict_entry = {'band_singer' : band_singer,\n",
    "        'ranking' : ranking,\n",
    "        'title' : title,\n",
    "        'url' : url}\n",
    "        #appends our new dictionary to our song_list\n",
    "        song_list.append(dict_entry)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check\n",
    "song_list[2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "You should get something similar to this (where songs is the aforementioned list):\n",
    "\n",
    "```\n",
    "songs[2:4]\n",
    "```\n",
    "\n",
    "```\n",
    "[{'band_singer': 'The Guess Who',\n",
    "  'ranking': 3,\n",
    "  'title': '\"American Woman\"',\n",
    "  'url': '/wiki/The_Guess_Who'},\n",
    " {'band_singer': 'B.J. Thomas',\n",
    "  'ranking': 4,\n",
    "  'title': '\"Raindrops Keep Fallin\\' on My Head\"',\n",
    "  'url': '/wiki/B.J._Thomas'}]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Generalize the previous: scrape Wikipedia from 1992 to 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By visiting the urls similar to the ones for 1970, we can obtain the billboard top 100 for the years 1992 to 2014. (We choose these later years rather than 1970 as you might find music from this era more interesting.) Download these using Python's `requests` module and store the text from those requests in a dictionary called `yearstext`. This dictionary ought to have as its keys the years (as integers from 1992 to 2014), and as values corresponding to these keys the text of the page being fetched.\n",
    "\n",
    "You ought to sleep a second (look up `time.sleep` in Python) at the very least in-between fetching each web page: you do not want Wikipedia to think you are a marauding bot attempting to mount a denial-of-service attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieves full webpages from wikipedia saving each to a dictionary keyed by year\n",
    "yearstext = {}\n",
    "for year in range(1992, 2015):\n",
    "    with urllib.request.urlopen(f'https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_%s' % year) as response:\n",
    "        year_text = {year : response.read()}\n",
    "        yearstext.update(year_text)\n",
    "        time.sleep(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*HINT: you might find `range` and string-interpolation useful to construct the URLs *.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Parse and Clean data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the code you wrote to get data from 1970 which produces a list of dictionaries, one corresponding to each single.  Now write a function `parse_year(the_year, yeartext_dict)` which takes the year, prints it out, gets the text for the year from the just created `yearstext` dictionary, and return a list of dictionaries for that year, with one dictionary for each single. Store this list in the variable `yearinfo`.\n",
    "\n",
    "The dictionaries **must** be of this form:\n",
    "\n",
    "```\n",
    "{'band_singer': ['Brandy', 'Monica'],\n",
    "  'ranking': 2,\n",
    "  'song': ['The Boy Is Mine'],\n",
    "  'songurl': ['/wiki/The_Boy_Is_Mine_(song)'],\n",
    "  'titletext': '\" The Boy Is Mine \"',\n",
    "  'url': ['/wiki/Brandy_Norwood', '/wiki/Monica_(entertainer)']}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spec of this function is provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "parse_year\n",
    "\n",
    "Inputs\n",
    "------\n",
    "the_year: the year you want the singles for\n",
    "yeartext_dict: a dictionary with keys as integer years and values the downloaded web pages \n",
    "    from wikipedia for that year.\n",
    "   \n",
    "Returns\n",
    "-------\n",
    "\n",
    "a list of dictionaries, each of which corresponds to a single and has the\n",
    "following data:\n",
    "\n",
    "\n",
    "Eg:\n",
    "\n",
    "{'band_singer': ['Brandy', 'Monica'],\n",
    "  'ranking': 2,\n",
    "  'song': ['The Boy Is Mine'],\n",
    "  'songurl': ['/wiki/The_Boy_Is_Mine_(song)'],\n",
    "  'titletext': '\" The Boy Is Mine \"',\n",
    "  'url': ['/wiki/Brandy_Norwood', '/wiki/Monica_(entertainer)']}\n",
    "  \n",
    "A dictionary with the following data:\n",
    "    band_singer: a list of bands/singers who made this single\n",
    "    song: a list of the titles of songs on this single\n",
    "    songurl: a list of the same size as song which has urls for the songs on the single \n",
    "        (see point 3 above)\n",
    "    ranking: ranking of the single\n",
    "    titletext: the contents of the table cell\n",
    "    band_singer: a list of bands or singers on this single\n",
    "    url: a list of wikipedia singer/band urls on this single: only put in the part \n",
    "        of the url from /wiki onwards\n",
    "    \n",
    "\n",
    "Notes\n",
    "-----\n",
    "See description and example above.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_year(the_year, yeartext_dict):\n",
    "    year = the_year\n",
    "    yearinfo = []\n",
    "    song = []\n",
    "    songurl = []\n",
    "    band_singer = []\n",
    "    title = []\n",
    "    url = []\n",
    "    title_text = ''\n",
    "    i = 0\n",
    "    title_string = ''\n",
    "    band_singer = ''\n",
    "    soup = BeautifulSoup(yearstext[year], \"lxml\")\n",
    "    tables = soup.find('table', attrs={'class':'wikitable sortable'})        \n",
    "    #iterates through tree structure, scraping our data\n",
    "    tr_list = tables.find_all('tr')    \n",
    "    for tr in tr_list:\n",
    "        td_list = tr.find_all('td')\n",
    "        if td_list == [] :\n",
    "            td_list = []\n",
    "        else : \n",
    "            ranking = tr.th.string\n",
    "            links = tr.td.findAll('a')\n",
    "            number_of_links = len(links)   \n",
    "            if number_of_links == 0:\n",
    "                songurl = [None]\n",
    "                title_text = [a['title']]\n",
    "                song.append(a['title'] )\n",
    "            else :\n",
    "                i = 0\n",
    "                for a in tr.td.findAll('a') : \n",
    "                    song.append(a['title'] )\n",
    "                    title_string = '\\\"' + a['title'] + '\\\"'    \n",
    "                    if i == 0 :\n",
    "                        title_text = title_string\n",
    "                        i = i + 1\n",
    "                    else :\n",
    "                        title_text = title_text + ' / ' + title_string\n",
    "                        i = i + 1    \n",
    "                    songurl.append(a['href'])\n",
    "            title = song\n",
    "            #finds next td tag\n",
    "            tr.td.findNext('td') \n",
    "            temp = len(tr.td.findNext('td').findAll('a'))\n",
    "            if temp == 0:\n",
    "                singer_url = [None]\n",
    "                band_singer = tr.td.findNext('td').string\n",
    "                band_singer = [band_singer]\n",
    "            elif temp == 1:\n",
    "                singer_url = tr.td.findNext('td').a['href']\n",
    "                singer_url = [singer_url]\n",
    "                band_singer = tr.td.findNext('td').a.string\n",
    "                band_singer = [band_singer]\n",
    "            else:\n",
    "                singer_url = []\n",
    "                band_singer = []\n",
    "                for a in tr.td.findNext('td').findAll('a'):\n",
    "                    webpage = a['href']\n",
    "                    singer_url.append(webpage)\n",
    "                    band_singer.append(a.string)            \n",
    "            #creates dictionary entry                   \n",
    "            dict_entry = {'band_singer' : band_singer,\n",
    "            'ranking' : ranking,\n",
    "            'song' : title, 'songurl': songurl, 'titletext' : title_text,\n",
    "            'url' : singer_url}\n",
    "            #appends new dictionary to our list\n",
    "            yearinfo.append(dict_entry)      \n",
    "            songurl = []\n",
    "            song = []\n",
    "            title_string = ''\n",
    "            title_text = ''    \n",
    "    return(yearinfo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse each year\n",
    "years_dict = {}\n",
    "for year in range(1992, 2015):\n",
    "    years_dict.update({year : parse_year(year, yearstext)})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#assign the complete dictionary back to yearinfo\n",
    "yearinfo = years_dict  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpful notes\n",
    "\n",
    "Notice that some singles might have multiple songs:\n",
    "\n",
    "```\n",
    "{'band_singer': ['Jewel'],\n",
    "  'ranking': 2,\n",
    "  'song': ['Foolish Games', 'You Were Meant for Me'],\n",
    "  'songurl': ['/wiki/Foolish_Games',\n",
    "   '/wiki/You_Were_Meant_for_Me_(Jewel_song)'],\n",
    "  'titletext': '\" Foolish Games \" / \" You Were Meant for Me \"',\n",
    "  'url': ['/wiki/Jewel_(singer)']}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And some singles don't have a song URL:\n",
    "\n",
    "```\n",
    "{'band_singer': [u'Nu Flavor'],\n",
    "  'ranking': 91,\n",
    "  'song': [u'Heaven'],\n",
    "  'songurl': [None],\n",
    "  'titletext': u'\"Heaven\"',\n",
    "  'url': [u'/wiki/Nu_Flavor']}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus there are some issues this function must handle:\n",
    "\n",
    "1. There can be more than one  `band_singer` as can be seen above (sometimes with a comma, sometimes with \"featuring\" in between). The best way to parse these is to look for the urls.\n",
    "2. There can be two songs in a single, because of the way the industry works: there are two-sided singles. See https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_1997 for an example. You can find other examples in 1998 and 1999.\n",
    "3. The `titletext` is the contents of the table cell, and retains the quotes that Wikipedia puts on the single.\n",
    "4. If no song anchor is found (see the 24th song in the above url), assume there is one song in the single, set `songurl` to [`None`] and the song name to the contents of the table cell with the quotes stripped (ie `song` is a one-element list with this the `titletext` stripped of its quotes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a check, we can do this for 1997. We'll print the first 5 outputs: `parse_year(1997, yearstext)[:5]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should give the following. Notice that the year 1997 exercises the edge cases we talked about earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "parse_year(1997, yearstext)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[{'band_singer': ['Elton John'],\n",
    "  'ranking': 1,\n",
    "  'song': ['Something About the Way You Look Tonight',\n",
    "   'Candle in the Wind 1997'],\n",
    "  'songurl': ['/wiki/Something_About_the_Way_You_Look_Tonight',\n",
    "   '/wiki/Candle_in_the_Wind_1997'],\n",
    "  'titletext': '\" Something About the Way You Look Tonight \" / \" Candle in the Wind 1997 \"',\n",
    "  'url': ['/wiki/Elton_John']},\n",
    " {'band_singer': ['Jewel'],\n",
    "  'ranking': 2,\n",
    "  'song': ['Foolish Games', 'You Were Meant for Me'],\n",
    "  'songurl': ['/wiki/Foolish_Games',\n",
    "   '/wiki/You_Were_Meant_for_Me_(Jewel_song)'],\n",
    "  'titletext': '\" Foolish Games \" / \" You Were Meant for Me \"',\n",
    "  'url': ['/wiki/Jewel_(singer)']},\n",
    " {'band_singer': ['Puff Daddy', 'Faith Evans', '112'],\n",
    "  'ranking': 3,\n",
    "  'song': [\"I'll Be Missing You\"],\n",
    "  'songurl': ['/wiki/I%27ll_Be_Missing_You'],\n",
    "  'titletext': '\" I\\'ll Be Missing You \"',\n",
    "  'url': ['/wiki/Sean_Combs', '/wiki/Faith_Evans', '/wiki/112_(band)']},\n",
    " {'band_singer': ['Toni Braxton'],\n",
    "  'ranking': 4,\n",
    "  'song': ['Un-Break My Heart'],\n",
    "  'songurl': ['/wiki/Un-Break_My_Heart'],\n",
    "  'titletext': '\" Un-Break My Heart \"',\n",
    "  'url': ['/wiki/Toni_Braxton']},\n",
    " {'band_singer': ['Puff Daddy', 'Mase'],\n",
    "  'ranking': 5,\n",
    "  'song': [\"Can't Nobody Hold Me Down\"],\n",
    "  'songurl': ['/wiki/Can%27t_Nobody_Hold_Me_Down'],\n",
    "  'titletext': '\" Can\\'t Nobody Hold Me Down \"',\n",
    "  'url': ['/wiki/Sean_Combs', '/wiki/Mase']}]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save a json file of information from the scraped files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not want to lose all this work, so let's save the last data structure we created to disk. That way if you need to re-run from here, you don't need to redo all these requests and parsing. \n",
    "\n",
    "DO NOT RERUN THE HTTP REQUESTS TO WIKIPEDIA WHEN SUBMITTING.\n",
    "\n",
    "*We **DO NOT** need to see these JSON files in your submission!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DO NOT RERUN THIS CELL WHEN SUBMITTING\n",
    "fd = open(\"yearinfo.json\",\"w+\")\n",
    "json.dump(yearinfo, fd)\n",
    "fd.close()\n",
    "del yearinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's reload our JSON file into the yearinfo variable, just to be sure everything is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RERUN WHEN SUBMITTING\n",
    "# Another way to deal with files. Has the advantage of closing the file for you.\n",
    "with open(\"yearinfo.json\", \"r\") as fd:\n",
    "    yearinfo = json.load(fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Construct a year-song-singer dataframe from the yearly information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct a dataframe `flatframe` from the `yearinfo`. The frame should be similar to the frame below.  Each row of the frame represents a song, and carries with it the chief properties of year, song, singer, and ranking.\n",
    "\n",
    "![](https://raw.githubusercontent.com/cs109/a-2017/master/hwassets/images/HW1SC1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turns our list into a list of dictionaries where each entry contains a year key  \n",
    "rows = []\n",
    "for year in yearinfo.keys():\n",
    "    for song in yearinfo[year]:\n",
    "        song['year'] = year\n",
    "        rows.append(song)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO RERUN THIS CELL, you must restart and clear all output!!!\n",
    "\n",
    "# the following code removes lists from inside of our dictionaries inside of our list. It then creates new dictionary entries\n",
    "# for each individual list item\n",
    "band_singer = []\n",
    "songurl = ''\n",
    "title_text = ''\n",
    "singer_url = []\n",
    "starting_length = len(rows)\n",
    "for dics in rows: \n",
    "    if starting_length == 0:\n",
    "        break\n",
    "    dict_add = {}   \n",
    "    # checks if our dictionary contains lists longer than one element\n",
    "    if len(dics['band_singer']) > 1:\n",
    "        i = 0\n",
    "        j = len(dics['band_singer'])\n",
    "        \n",
    "        for value in dics['band_singer']:\n",
    "            # new dictionary entry to append to our list\n",
    "            dict_add = {'band_singer' : dics['band_singer'][i],\n",
    "            'ranking' : dics['ranking'],\n",
    "            'song' : dics['song'], 'songurl': dics['songurl'], 'titletext' : dics['titletext'],\n",
    "            'url' : dics['url'][i], 'year' : dics['year']}\n",
    "            rows.append(dict_add)\n",
    "            i = 1 + i\n",
    "            j = j - 1\n",
    "    starting_length = starting_length - 1\n",
    "\n",
    "rows2 = []\n",
    "band_singer = []\n",
    "\n",
    "# loops through our list again, removing duplicate entries\n",
    "for dics in rows:\n",
    "    if len(dics['band_singer']) == 1 or len(dics['band_singer']) > 5:\n",
    "        rows2.append(dics)\n",
    "\n",
    "# turns all remaining one element lists into strings\n",
    "for row in rows2:\n",
    "    for key in row:\n",
    "        row[key] = str(row[key])\n",
    "        row[key] = row[key].strip(\"[]\")\n",
    "        row[key] = row[key].strip(\"''\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct the dataframe, we'll need to iterate over the years and the singles per year. Notice how, above, the dataframe is ordered by ranking and then year. While the exact order is up to you, note that you will have to come up with a scheme to order the information.\n",
    "\n",
    "Check that the dataframe has sensible data types. You will also likely find that the year field has become an \"object\" (Pandas treats strings as generic objects): this is due to the conversion to and back from JSON. Such conversions need special care. Fix any data type issues with `flatframe`. (See Pandas [astype](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.astype.html) function.) \n",
    "We will use this `flatframe` in the next question. \n",
    "\n",
    "(As an aside, we used the name `flatframe` to indicate that this dataframe is flattened from a hierarchical dictionary structure with the keys being the years.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe\n",
    "flatframe = pd.DataFrame(rows2)\n",
    "\n",
    "\n",
    "# check datatypes of dataframe columns\n",
    "flatframe['year'].dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "flatframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert year column to int\n",
    "flatframe['year'] = flatframe['year'].astype(np.uint16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who are the highest quality singers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show the highest quality singers and plot them on a bar chart.\n",
    "\n",
    "#### 1.5 Find highest quality singers according to how prolific they are\n",
    "\n",
    "What do we mean by highest quality? This is of course open to interpretation, but let's define \"highest quality\" here as the number of times a singer appears in the top 100 over this time period. If a singer appears twice in a year (for different songs), this is counted as two appearances, not one. \n",
    "\n",
    "Make a bar-plot of the most prolific singers. Singers on this chart should have appeared at-least more than 15 times. (HINT: look at the docs for the pandas method `value_counts`.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count artist appearences in top 100\n",
    "\n",
    "\n",
    "\n",
    "artist_count = flatframe[\"band_singer\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot top artists\n",
    "artist_count[:8].plot.barh()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 What if we used a different metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_df = pd.DataFrame(np.array(flatframe[['band_singer', 'song', 'ranking']]))\n",
    "scored_df.columns = ['band_singer', 'song', 'ranking']\n",
    "# scored_df.groupby(['band_singer', 'song'])['ranking']\n",
    "\n",
    "scored_df['ranking'] = scored_df['ranking'].apply(lambda x: 101 - int(x))\n",
    "\n",
    "#display(scored_df)\n",
    "\n",
    "scored_df.groupby('band_singer')['ranking'].aggregate(np.sum).sort_values(ascending=False).head(20).plot.barh()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we would like to capture is this: a singer should to be scored higher if the singer appears higher in the rankings. So we'd say that a singer who appeared once at a higher and once at a lower ranking is a \"higher quality\" singer than one who appeared twice at a lower ranking. \n",
    "\n",
    "To do this, group all of a singers songs together and assign each song a score `101 - ranking`. Order the singers by their total score and make a bar chart for the top 20.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 Do you notice any major differences when you change the metric?\n",
    "\n",
    "How have the singers at the top shifted places? Why do you think this happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "While Rihanna keeps her place at the head of the pack (by a sizeable margin, it should be noted), \n",
    "Mariah Carey gains materially over R. Kelly, Ludacris and Lil Wayne. The reason for this is that while\n",
    "Mariah's songs appear in the top 100 less frequently than those other three, the latter chart weighs\n",
    "the relative rank of her songs while the former disregards this and looks at only frequency. Including\n",
    "relative rank also gives Katy Perry a place near the top, too.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Scraping and Constructing: Information about Artists, Bands and Genres from Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next job is to use those band/singer urls we collected under `flatframe.url` and get information about singers and/or bands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape information about artists from wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to fetch information about the singers or groups for all the winning songs in a list of years.\n",
    "\n",
    "Here we show a function that fetches information about a singer or group from their url on wikipedia. We create a cache object `urlcache` that will avoid redundant HTTP requests (e.g. an artist might have multiple singles on a single year, or be on the list over a span of years). Once we have fetched information about an artist, we don't need to do it again. The caching also helps if the network goes down, or the target website is having some problems. You simply need to run the `get_page` function below again, and the `urlcache` dictionary will continue to be filled.\n",
    "\n",
    "If the request gets an HTTP return code different from 200, (such as a 404 not found or 500 Internal Server Error) the cells for that URL will have a value of 1; and if the request completely fails (e.g. no network connection) the cell will have a value of 2. This will allow you to analyse the failed requests.\n",
    "\n",
    "Notice that we have wrapped the call in whats called _an exception block_. We try to make the request. If it fails entirely, or returns a HTTP code thats not 200, we set the status to 2 and 1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urlcache={}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    # Check if URL has already been visited.\n",
    "    if (url not in urlcache) or (urlcache[url]==1) or (urlcache[url]==2):\n",
    "        time.sleep(1)\n",
    "        # try/except blocks are used whenever the code could generate an exception (e.g. division by zero).\n",
    "        # In this case we don't know if the page really exists, or even if it does, if we'll be able to reach it.\n",
    "        try:\n",
    "            r = requests.get(\"http://en.wikipedia.org%s\" % url)\n",
    "\n",
    "            if r.status_code == 200:\n",
    "                urlcache[url] = r.text\n",
    "            else:\n",
    "                urlcache[url] = 1\n",
    "        except:\n",
    "            urlcache[url] = 2\n",
    "    return urlcache[url]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sort the `flatframe` by year, ascending, first. Think why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sort the flatframe by year\n",
    "flatframe = flatframe.sort_values('year')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pulling and saving the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have to run this function again and again, in case there were network problems. Note that, because there is a \"global\" cache, it will take less time each time you run it. Also note that this function is designed to be run again and again: it attempts to make sure that there are no unresolved pages remaining. Let us make sure of this: *the sum below should be 0, and the boolean True.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT RERUN THIS CELL WHEN SUBMITTING\n",
    "# Here we are populating the url cache\n",
    "# subsequent calls to this cell should be very fast, since Python won't\n",
    "# need to fetch the page from the web server.\n",
    "# NOTE this function will take quite some time to run (about 30 mins for me), since we sleep 1 second before\n",
    "# making a request. If you run it again it will be almost instantaneous, save requests that might have failed\n",
    "# (you will need to run it again if requests fail..see cell below for how to test this)\n",
    "flatframe[\"url\"].apply(get_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT RERUN THIS CELL WHEN SUBMITTING\n",
    "print(\"Number of bad requests:\",np.sum([(urlcache[k]==1) or (urlcache[k]==2) for k in urlcache])) # no one or 0's)\n",
    "print(\"Did we get all urls?\", len(flatframe.url.unique())==len(urlcache)) # we got all of the urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the `urlcache` to disk, just in case we need it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RERUN THIS CELL WHEN SUBMITTING\n",
    "with open(\"artistinfo.json\",\"w\") as fd:\n",
    "    json.dump(urlcache, fd)\n",
    "del urlcache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RERUN WHEN SUBMITTING\n",
    "with open(\"artistinfo.json\") as json_file:\n",
    "    urlcache = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Extract information about singers and bands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From each page we collected about a singer or a band, extract the following information:\n",
    "\n",
    "1. If the page has the text \"Born\" in the sidebar on the right, extract the element with the class `.bday`. If the page doesn't contain \"Born\", store `False`.  Store either of these into the variable `born`. We want to analyze the artist's age.\n",
    "\n",
    "2. If the text \"Years active\" is found, but no \"born\", assume a band. Store into the variable `ya` the value of the next table cell corresponding to this, or `False` if the text is not found.\n",
    "\n",
    "Put this all into a function `singer_band_info` which takes the singer/band url as argument and returns a dictionary `dict(url=url, born=born, ya=ya)`.\n",
    "\n",
    "The information can be found on the sidebar on each such wikipedia page, as the example here shows:\n",
    "\n",
    "![sandg](https://raw.githubusercontent.com/cs109/a-2017/master/hwassets/images/sandg.png).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the function `singer_band_info` according to the following specification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "singer_band_info\n",
    "\n",
    "Inputs\n",
    "------\n",
    "url: the url\n",
    "page_text: the text associated with the url\n",
    "   \n",
    "Returns\n",
    "-------\n",
    "A dictionary with the following data:\n",
    "    url: copy the input argument url into this value\n",
    "    born: the artist's birthday\n",
    "    ya: years active variable\n",
    "\n",
    "Notes\n",
    "-----\n",
    "See description above. Also note that some of the genres urls might require a \n",
    "bit of care and special handling.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def singer_band_info(url, page_text):\n",
    "    bday_dict = {}\n",
    "    bday = ''\n",
    "    ya = ''\n",
    "    # soupify our webpage\n",
    "    soup = BeautifulSoup(page_text[url], \"lxml\")\n",
    "    tables = soup.find('table', attrs={'class':'infobox vcard plainlist'})\n",
    "    if (tables == None):\n",
    "        tables = soup.find('table', attrs={'class':'infobox biography vcard'})\n",
    "    bday = tables.find('span', {'class': 'bday'})\n",
    "    if bday: \n",
    "        bday = bday.get_text()[:4]\n",
    "        bday_dict = {'url' : url, 'born' : bday, 'ya' : ya}\n",
    "    else:\n",
    "        ya = False\n",
    "        for tr in tables.find_all('tr'):\n",
    "            if hasattr(tr.th, 'span'):\n",
    "                if hasattr(tr.th.span, 'string'):\n",
    "                    if tr.th.span.string == 'Years active':                \n",
    "                        if hasattr(tr.th, 'span'):\n",
    "                            #ya = tr.td.string\n",
    "                            ya = tr.td.text   #DK add\n",
    "                            bday = 'False'\n",
    "                            bday_dict = {'url' : url, 'born' : 'False', 'ya' : ya}\n",
    "    return(bday_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = '/wiki/Iggy_Azalea'\n",
    "singer_band_info(url, urlcache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create additional dictionary with all artists additional information with their url as a key\n",
    "list_of_addit_dicts = []\n",
    "bday_dict = {}\n",
    "for url in urlcache.keys():   \n",
    "    try:\n",
    "        bday_dict = singer_band_info(url, urlcache)\n",
    "        list_of_addit_dicts.append(bday_dict)\n",
    "    except:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_addit_dicts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the additional dictionary to create additional dataframe\n",
    "\n",
    "additional_df = pd.DataFrame(list_of_addit_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the two dataframes\n",
    "\n",
    "largedf = pd.merge(flatframe, additional_df, left_on='url', right_on='url', how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largedf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2  Merging this information in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over the items in the singer-group dictionary cache `urlcache`, run the above function, and create a dataframe from there with columns `url`, `born`, and `ya`. Merge this dataframe on the `url` key with `flatframe`, creating a rather wide dataframe that we shall call `largedf`. It should look something like this:\n",
    "\n",
    "![](https://raw.githubusercontent.com/cs109/a-2017/master/hwassets/images/HW1SC3.png)\n",
    "\n",
    "Notice how the `born` and `ya` and `url` are repeated every time a different song from a given band is represented in a row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 What is the age at which singers achieve their top ranking?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a histogram of the age at which singers achieve their top ranking. What conclusions can you draw from this distribution of ages?\n",
    "\n",
    "*HINT: You will need to do some manipulation of the `born` column, and find the song for which a band or an artist achieves their top ranking. You will then need to put these rows together into another dataframe or array to make the plot.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(np.array(largedf[['year', 'band_singer', 'ranking', 'born']]))\n",
    "new_df.columns = ['year', 'band_singer', 'ranking', 'born']\n",
    "new_df['born'] = pd.to_numeric(new_df['born'], errors='coerce').fillna(0).astype(np.int64)                     #convert to int\n",
    "new_df['year_minus_born'] = new_df['year'] - new_df['born']\n",
    "sorted_df = new_df.sort_values(['band_singer', 'ranking']);\n",
    "filtered_df = sorted_df.drop_duplicates(subset='band_singer', keep='first')\n",
    "filtered_df = filtered_df.query('born > 0')\n",
    "print(filtered_df)\n",
    "filtered_df.groupby('band_singer')['year_minus_born'].aggregate(np.sum).sort_values(ascending=False).plot.hist(bins=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 At what year since inception do bands reach their top rankings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a similar calculation to plot a histogram of the years since inception at which bands reach their top ranking. What conclusions can you draw?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_2 = pd.DataFrame(np.array(largedf[['year', 'band_singer', 'ranking', 'ya']]))\n",
    "new_df_2.columns = ['year', 'band_singer', 'ranking', 'ya']\n",
    "new_df_2['ya'] = new_df_2['ya'].str[:4]\n",
    "new_df_2['ya'] = pd.to_numeric(new_df_2['ya'], errors='coerce').fillna(0).astype(np.int64)                     #convert to int\n",
    "new_df_2['year_minus_ya'] = new_df_2['year'] - new_df_2['ya']\n",
    "sorted_df_2 = new_df_2.sort_values(['band_singer', 'ranking']);\n",
    "filtered_df_2 = sorted_df_2.drop_duplicates(subset='band_singer', keep='first')\n",
    "filtered_df_2 = filtered_df_2.query('ya > 0')\n",
    "filtered_df_2 = filtered_df_2.query('year_minus_ya < 40')\n",
    "print(filtered_df_2)\n",
    "filtered_df_2.groupby('band_singer')['year_minus_ya'].aggregate(np.sum).sort_values(ascending=False).plot.hist(bins=10)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "from matplotlib import rcParams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def cv_optimize(clf, parameters, X, y, n_jobs=1, n_folds=5, score_func=None):\n",
    "    if score_func:\n",
    "        gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds, n_jobs=n_jobs, scoring=score_func)\n",
    "    else:\n",
    "        gs = GridSearchCV(clf, param_grid=parameters, n_jobs=n_jobs, cv=n_folds)\n",
    "    gs.fit(X, y)\n",
    "    print (\"BEST\", gs.best_params_, gs.best_score_, gs.cv_result_)\n",
    "    best = gs.best_estimator_\n",
    "    return best\n",
    "\n",
    "def do_classify(clf, parameters, indf, featurenames, targetname, target1val, mask=None, reuse_split=None, score_func=None, n_folds=5, n_jobs=1):\n",
    "    subdf=indf[featurenames]\n",
    "    X=subdf.values\n",
    "    y=(indf[targetname].values==target1val)*1\n",
    "    if mask is not None:\n",
    "        print(\"using mask\")\n",
    "        Xtrain, Xtest, ytrain, ytest = X[mask], X[~mask], y[mask], y[~mask]\n",
    "    if reuse_split is not None:\n",
    "        print(\"using reuse split\")\n",
    "        Xtrain, Xtest, ytrain, ytest = reuse_split['Xtrain'], reuse_split['Xtest'], reuse_split['ytrain'], reuse_split['ytest']\n",
    "    if parameters:\n",
    "        clf = cv_optimize(clf, parameters, Xtrain, ytrain, n_jobs=n_jobs, n_folds=n_folds, score_func=score_func)\n",
    "    clf=clf.fit(Xtrain, ytrain)\n",
    "    training_accuracy = clf.score(Xtrain, ytrain)\n",
    "    test_accuracy = clf.score(Xtest, ytest)\n",
    "    print (\"############# based on standard predict ################\")\n",
    "    print (\"Accuracy on training data: %0.2f\" % (training_accuracy))\n",
    "    print (\"Accuracy on test data:     %0.2f\" % (test_accuracy))\n",
    "    print (confusion_matrix(ytest, clf.predict(Xtest)))\n",
    "    print (\"########################################################\")\n",
    "    return clf, Xtrain, ytrain, Xtest, ytest\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #\n",
    "# Plot tree containing only two covariates\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "# cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "def plot_2tree(ax, Xtr, Xte, ytr, yte, clf, plot_train = True, plot_test = True, lab = ['Feature 1', 'Feature 2'], mesh=True, colorscale=cmap_light, cdiscrete=cmap_bold, alpha=0.3, psize=10, zfunc=False):\n",
    "    # Create a meshgrid as our test data\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plot_step= 0.05\n",
    "    xmin, xmax= Xtr[:,0].min(), Xtr[:,0].max()\n",
    "    ymin, ymax= Xtr[:,1].min(), Xtr[:,1].max()\n",
    "    xx, yy = np.meshgrid(np.arange(xmin, xmax, plot_step), np.arange(ymin, ymax, plot_step) )\n",
    "\n",
    "    # Re-cast every coordinate in the meshgrid as a 2D point\n",
    "    Xplot= np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "\n",
    "    # Predict the class\n",
    "    Z = clfTree1.predict( Xplot )\n",
    "\n",
    "    # Re-shape the results\n",
    "    Z= Z.reshape( xx.shape )\n",
    "    cs = plt.contourf(xx, yy, Z, cmap= cmap_light, alpha=0.3)\n",
    "  \n",
    "    # Overlay training samples\n",
    "    if (plot_train == True):\n",
    "        plt.scatter(Xtr[:, 0], Xtr[:, 1], c=ytr-1, cmap=cmap_bold, alpha=alpha,edgecolor=\"k\") \n",
    "    # and testing points\n",
    "    if (plot_test == True):\n",
    "        plt.scatter(Xte[:, 0], Xte[:, 1], c=yte-1, cmap=cmap_bold, alpha=alpha, marker=\"s\")\n",
    "\n",
    "    plt.xlabel(lab[0])\n",
    "    plt.ylabel(lab[1])\n",
    "    plt.title(\"Boundary for decision tree classifier\",fontsize=7.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('winequality-red.csv', sep=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 10.,   0.,  53.,   0., 681.,   0., 638.,   0., 199.,  18.]),\n",
       " array([3. , 3.5, 4. , 4.5, 5. , 5.5, 6. , 6.5, 7. , 7.5, 8. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "\n",
    "plt.hist(df.quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\home\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "Y = df['quality'].values\n",
    "df_tmp = df.drop('quality',1)\n",
    "Y = np.array([1 if y>=7 else 0 for y in Y])\n",
    "X = df_tmp.as_matrix()\n",
    "\n",
    "df['target'] = (df['quality'].values >=7)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnames = list(df.columns.values[1:11])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1357098186366479"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test/train mask\n",
    "itrain, itest = train_test_split(range(df.shape[0]), train_size=0.6)\n",
    "mask=np.ones(df.shape[0], dtype='int')\n",
    "mask[itrain]=1\n",
    "mask[itest]=0\n",
    "mask = (mask==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Good wines in Training: 0.14911366006256518\n",
      "% Good wines in Testing: 0.115625\n"
     ]
    }
   ],
   "source": [
    "# make sure we didn't get unlucky in our mask selection\n",
    "print(\"% Good wines in Training:\", np.mean(df.target[mask]))\n",
    "print(\"% Good wines in Testing:\", np.mean(df.target[~mask]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\home\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'cv_result_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-febe42a67841>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m clfTree1, Xtrain, ytrain, Xtest, ytest = do_classify(clfTree1, parameters, df, \n\u001b[0;32m      6\u001b[0m                                                      \u001b[1;33m[\u001b[0m\u001b[1;34m'alcohol'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fixed acidity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                                                      mask=mask, n_jobs = 4, score_func = 'f1')\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-a08d229a52bd>\u001b[0m in \u001b[0;36mdo_classify\u001b[1;34m(clf, parameters, indf, featurenames, targetname, target1val, mask, reuse_split, score_func, n_folds, n_jobs)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreuse_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Xtrain'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreuse_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Xtest'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreuse_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ytrain'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreuse_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ytest'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv_optimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mclf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mtraining_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-a08d229a52bd>\u001b[0m in \u001b[0;36mcv_optimize\u001b[1;34m(clf, parameters, X, y, n_jobs, n_folds, score_func)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"BEST\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_result_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbest\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'cv_result_'"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "clfTree1 = tree.DecisionTreeClassifier()\n",
    "\n",
    "parameters = {\"max_depth\": [1, 2, 3, 4, 5, 6, 7], 'min_samples_leaf': [1, 2, 3, 4, 5, 6]}\n",
    "clfTree1, Xtrain, ytrain, Xtest, ytest = do_classify(clfTree1, parameters, df, \n",
    "                                                     ['alcohol', 'fixed acidity'],'target', 1, \n",
    "                                                     mask=mask, n_jobs = 4, score_func = 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2tree(plt, Xtrain, Xtest, ytrain, ytest, clfTree1, \n",
    "           lab = ['alcohol', 'fixed acidity'], alpha = 1, plot_test = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's plot the actual test data as well (in squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2tree(plt, Xtrain, Xtest, ytrain, ytest, clfTree1, \n",
    "           lab = ['alcohol', 'fixed acidity'], alpha = 1, plot_train = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's play around with this to see the possibly decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\home\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'cv_result_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-3302ab9d9cb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m clfTree_temp, Xtrain, ytrain, Xtest, ytest = do_classify(clfTree_temp, parameters, df, \n\u001b[0;32m      6\u001b[0m                                                      \u001b[1;33m[\u001b[0m\u001b[1;34m'alcohol'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fixed acidity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                                                      mask=mask, n_jobs = 4, score_func = 'f1')\n\u001b[0m\u001b[0;32m      8\u001b[0m plot_2tree(plt, Xtrain, Xtest, ytrain, ytest, clfTree_temp, \n\u001b[0;32m      9\u001b[0m            lab = ['alcohol', 'fixed acidity'], alpha = 1, plot_train = True)\n",
      "\u001b[1;32m<ipython-input-13-a08d229a52bd>\u001b[0m in \u001b[0;36mdo_classify\u001b[1;34m(clf, parameters, indf, featurenames, targetname, target1val, mask, reuse_split, score_func, n_folds, n_jobs)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreuse_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Xtrain'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreuse_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Xtest'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreuse_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ytrain'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreuse_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ytest'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv_optimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mclf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mtraining_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-a08d229a52bd>\u001b[0m in \u001b[0;36mcv_optimize\u001b[1;34m(clf, parameters, X, y, n_jobs, n_folds, score_func)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"BEST\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_result_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbest\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'cv_result_'"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "clfTree_temp = tree.DecisionTreeClassifier()\n",
    "\n",
    "parameters = {\"max_depth\": [None], 'min_samples_leaf': [4, 5, 6]}\n",
    "clfTree_temp, Xtrain, ytrain, Xtest, ytest = do_classify(clfTree_temp, parameters, df, \n",
    "                                                     ['alcohol', 'fixed acidity'],'target', 1, \n",
    "                                                     mask=mask, n_jobs = 4, score_func = 'f1')\n",
    "plot_2tree(plt, Xtrain, Xtest, ytrain, ytest, clfTree_temp, \n",
    "           lab = ['alcohol', 'fixed acidity'], alpha = 1, plot_train = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Predictors\n",
    "\n",
    "The prediction accuracy says we're doing quite well, but it seems like that's being driven by having such a large number of not excellent wines, even when optimizing the $F1$ score. Let's see if we can't increase the including all the covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfTree2 = tree.DecisionTreeClassifier()\n",
    "\n",
    "parameters = {\"max_depth\": [1, 2, 3, 4, 5, 6, 7], 'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "clfTree2, Xtrain, ytrain, Xtest, ytest = do_classify(clfTree2, parameters, df, \n",
    "                                                     Xnames,'target', 1, \n",
    "                                                     mask=mask, n_jobs = 4, score_func = 'f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Random Forests\n",
    "\n",
    "Recall from class that the random forest technique works by aggregating the results from a number of randomly perturbed decision trees constructed to explain the data.\n",
    "\n",
    "### A bit on bootstrap aggregation\n",
    "The idea of random forests arises naturally by first considering Tree bagging. In tree bagging we do the following $b$ times:\n",
    "\n",
    "1. Take a random subsample of your data\n",
    "2. Build a classification (or regression) tree like in the previous section\n",
    "3. repeat\n",
    "\n",
    "For a new data point we can then simply run that point through all the $b$ trees constructed, get all the decisions $\\hat{Y}_1,..., \\hat{Y}_b$ and take a majority vote. This form of averaging gets rid of some of the over-fitting issues found in just using one tree. Plus fitting these trees costs a lot computationally, so what else can we do?\n",
    "\n",
    "### Leads to Random Forests?\n",
    "This method is very similar to the bootstrap aggregation method. However, as the name suggests some extra randomness is injected into the building of the trees. It turns out that the trees that are build from the random subsample of your data are quite similar, so the solution is quite simple. In Random Forests we do the following $b$ times:\n",
    "\n",
    "1. Take a random subsample of your data\n",
    "2. randomly select a subset of predictors to be used in building the tree\n",
    "3. Build a classification (or regression) tree with only those variables selected in 2\n",
    "4. repeat\n",
    "\n",
    "We take a majority vote the same as before. Have a look at the help page for the [Random Forest Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), we'll be optimizing two options: `n_estimators` - the number of trees in the forest, `max_features` - the number of features to consider when looking for the best split (step 2 above).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clfForest = RandomForestClassifier()\n",
    "\n",
    "parameters = {\"n_estimators\": range(1, 20)}\n",
    "clfForest, Xtrain, ytrain, Xtest, ytest = do_classify(clfForest, parameters, \n",
    "                                                       df, Xnames, 'target', 1, mask=mask, \n",
    "                                                       n_jobs = 4, score_func='f1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Importance\n",
    "We can get a measure of how important a variable is from a random forest, it's essentially a measure of how well each particular variable is able to predict well when it is selected, for more on this and other details check out [this webpage](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#varimp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clfForest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-dcfb385b4d8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimportance_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclfForest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mname_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mimportance_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimportance_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbarh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimportance_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malign\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'center'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clfForest' is not defined"
     ]
    }
   ],
   "source": [
    "importance_list = clfForest.feature_importances_\n",
    "name_list = df.columns\n",
    "importance_list, name_list = zip(*sorted(zip(importance_list, name_list)))\n",
    "plt.barh(range(len(name_list)),importance_list,align='center')\n",
    "plt.yticks(range(len(name_list)),name_list)\n",
    "plt.xlabel('Relative Importance in the Random Forest')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Relative importance of Each Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. More Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods generalize this idea of putting many weak classifiers together (in a clever way) to build very accurate predictors. We saw this before with Random Forests and Bagging in the previous section. In this section we'll concentrate on a few particular Ensemble Methods\n",
    "\n",
    "## AdaBoost Classifier\n",
    "AdaBoost is short for Adaptive Boosting, the idea is quite simple. First you have some base classifier, like a Decision tree for example. \n",
    "\n",
    "First you assign to each of your n data points a weight of $w_i = 1/n$, then you do:\n",
    "\n",
    "1. fit that classifier (e.g. tree) on all your data, weighted by $w_i$\n",
    "2. test to see how well your classifier predicts your data\n",
    "3. data points that are miss-classified get a slightly higher weight, but update all the weights to $w_{i, new} = w_{i} \\exp({\\alpha_{t, x_i, y_i}})$\n",
    "\n",
    "In step 1, the way this weighting is done is by considering a weighted loss. For the details of this check out the Wikipedia article on [AdaBoost](https://en.wikipedia.org/wiki/AdaBoost#Example_algorithm_.28Discrete_AdaBoost.29). The only detail that is important to us now is that ${\\alpha_{t, x_i, y_i}}$ is some number that is large when $y_i$ is correctly classified and small otherwise.\n",
    "\n",
    "What this means is that every successive classifier (tree) puts more emphasis on classifying data points that the previous classifier missed. In this way each classifier can concentrate on a subset of the data. See the [help page](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) for more tuning parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\home\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'cv_result_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-60d74e86936c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m clfAda, Xtrain, ytrain, Xtest, ytest = do_classify(clfAda, parameters, \n\u001b[0;32m      7\u001b[0m                                                        \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'target'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                                                        n_jobs = 4, score_func='f1')\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-a08d229a52bd>\u001b[0m in \u001b[0;36mdo_classify\u001b[1;34m(clf, parameters, indf, featurenames, targetname, target1val, mask, reuse_split, score_func, n_folds, n_jobs)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreuse_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Xtrain'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreuse_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Xtest'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreuse_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ytrain'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreuse_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ytest'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv_optimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mclf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mtraining_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-a08d229a52bd>\u001b[0m in \u001b[0;36mcv_optimize\u001b[1;34m(clf, parameters, X, y, n_jobs, n_folds, score_func)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"BEST\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_result_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbest\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'cv_result_'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clfAda = AdaBoostClassifier()\n",
    "\n",
    "parameters = {\"n_estimators\": range(10, 60)}\n",
    "clfAda, Xtrain, ytrain, Xtest, ytest = do_classify(clfAda, parameters, \n",
    "                                                       df, Xnames, 'target', 1, mask=mask, \n",
    "                                                       n_jobs = 4, score_func='f1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier is also an ensemble method with trees as its base classifier, like AdaBoost it creates a sequence of classifiers that 'correct each other' sequentially. \n",
    "\n",
    "If you want to read more about this, and other Ensemble methods, check these places out:\n",
    "\n",
    "- An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani (see header)\n",
    "- [Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/) by Trevor Hastie, Robert Tibshirani, Jerome Friedman\n",
    "- [Gradient Boosting](https://en.wikipedia.org/wiki/Gradient_boosting)\n",
    "- [Boosting](https://en.wikipedia.org/wiki/Boosting_(machine_learning))\n",
    "- [Ensemble Learning](https://en.wikipedia.org/wiki/Ensemble_learning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clfGB = GradientBoostingClassifier()\n",
    "\n",
    "parameters = {\"n_estimators\": range(30, 60), \"max_depth\": [1, 2, 3, 4, 5]}\n",
    "clfGB, Xtrain, ytrain, Xtest, ytest = do_classify(clfGB, parameters, \n",
    "                                                       df, Xnames, 'target', 1, mask=mask, \n",
    "                                                       n_jobs = 4, score_func='f1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Classifiers Using Decision Surfaces\n",
    "Using this function and the results from the \"importance\" analysis above, **subset** the data matrix to include just the **two features of highest importance**. Then **plot** the decision surfaces of a <a href='http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier'>decision tree classifier</a>,  and a random forest classifier with **number of trees set to 15**, and a <a href='http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC'> support vector machine</a> **with `C` set to 100, and `gamma` set to 1.0**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import sklearn.linear_model\n",
    "import sklearn.svm\n",
    "\n",
    "def plot_decision_surface(clf, X_train, Y_train):\n",
    "    plot_step=0.1\n",
    "    \n",
    "    if X_train.shape[1] != 2:\n",
    "        raise ValueError(\"X_train should have exactly 2 columnns!\")\n",
    "    \n",
    "    x_min, x_max = X_train[:, 0].min() - plot_step, X_train[:, 0].max() + plot_step\n",
    "    y_min, y_max = X_train[:, 1].min() - plot_step, X_train[:, 1].max() + plot_step\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "    clf.fit(X_train,Y_train)\n",
    "    if hasattr(clf, 'predict_proba'):\n",
    "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n",
    "    else:\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])    \n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Reds)\n",
    "    plt.scatter(X_train[:,0],X_train[:,1],c=Y_train,cmap=plt.cm.Paired)\n",
    "    plt.show()\n",
    "    \n",
    "# your code here\n",
    "imp_cols = clfForest.feature_importances_.argsort()[::-1][0:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_imp = df[imp_cols].values\n",
    "Y = df.target.values\n",
    "\n",
    "classifiers = [clfTree2,\n",
    "               clfForest,\n",
    "               clfAda,\n",
    "               clfGB,\n",
    "               sklearn.svm.SVC(C=100.0, gamma=1.0)]\n",
    "\n",
    "titleClassifer = ['Decision Tree Classifier', 'Random Forest Classifier', \n",
    "                  'AdaBoost Classifier', 'Gradient Boosting Classifier', 'Support Vector Machine']\n",
    "for c in xrange(4):\n",
    "    plt.title(titleClassifer[c])\n",
    "    plt.xlabel(name_list[0])\n",
    "    plt.ylabel(name_list[1])\n",
    "    plot_decision_surface(classifiers[c], X_imp, df.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
